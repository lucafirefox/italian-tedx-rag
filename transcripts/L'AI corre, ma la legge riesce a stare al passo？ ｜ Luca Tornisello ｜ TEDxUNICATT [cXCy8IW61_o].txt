 C'è una foto del marzo 2023 che secondo me racconta molto bene come il mondo sta cambiando. Ed è questa. Il Papa col piumino bianco. Quello che succede è che un giorno Pablo Xavier, un operaio di Chicago, usa un programma di intelligenza artificiale generativa che produce immagini a partire da un testo e dà vita a questa foto, falsa ma iperrealistica, che fa il giro del mondo. Ciò che preoccupa non è tanto la svolta trendy, quasi stile trapper del pontefice, ma ciò che è una tecnologia ormai così accessibile da chiunque può produrre in termini di informazione. Il Times scrive, la storia potrebbe ricordare il Papa col piumino bianco come il primo evento di disinformazione veramente virale alimentato dalla tecnologia deepfake. È un segno del peggio che verrà. Ora, potremmo discutere per ore di quanto sarà peggio ciò che verrà. Ciò che è certo è che l'intelligenza artificiale sta cambiando il mondo, il modo in cui viviamo, interagiamo, il modo in cui lavoriamo. Lo fa appunto col deepfake, con la diffusione di video e di voci falsi, di politici, che plasmano le nostre scelte di voto disinformate, fanno vacillare le nostre democrazie, specialmente in un anno elettorale come questo. Cambia per sempre la vita delle persone col deepfake porn, la diffusione di materiale sessualmente esplicito generato artificialmente. L'intelligenza artificiale cambia il volto delle guerre, della musica, della scuola, delle nostre case. Rivoluziona la sanità e le diagnosi mediche, ma anche l'agricoltura, il commercio, la logistica. Insomma, ciò che è certo è che l'intelligenza artificiale avanza tanto velocemente. Quello che dobbiamo comprendere è in che direzione stiamo andando, o meglio, in che direzione vogliamo andare. E per chi come me ama il diritto è proprio quella velocità il tema. A che cosa serve il diritto? Prima lezione di giurisprudenza. Il diritto ha lo scopo di disciplinare i rapporti tra gli esseri umani in un determinato momento storico, o meglio, fornisce delle regole, dette norme giuridiche, che i membri di una società come noi si impegnano a rispettare perché quella società si mantenga organizzata e più o meno pacifica. E nel disciplinare i rapporti tra gli umani oggi il diritto disciplina la tecnologia perché le nostre interazioni passano attraverso la tecnologia. E qui sorge un problema. Un problema che nel 2009 Larry Downs in The Laws of Disruption ha definito il problema del ritmo. E cioè, mentre la tecnologia avanza esponenzialmente, i sistemi giuridici procedono progressivamente. Cioè, da un lato c'è la tecnologia, l'intelligenza artificiale, che accelera, lo fa molto velocemente, e dall'altro la legge che fa l'esatto opposto. Procede a passo d'uomo, viaggia a velocità costante. Un giurista americano ha scritto che nel duello tra tecnologia e legge spesso la tecnologia ha già vinto prima ancora che la legge abbia iniziato a combattere. Ecco, questa frase riflette bene questa disparità tra due velocità, tra l'intelligenza artificiale che corre e la legge che la rincorre per cercare di regolarla e di disciplinarla. Questa discrepanza temporale può portare a delle situazioni di vuoto normativo, cioè a delle situazioni in cui non ci sono regole, in cui le aziende possono procedere senza che nessuno dica fermiamoci e capiamo in che direzione stiamo andando. Vi voglio raccontare di due aspetti di questa immensa sfida. Quello etico e quello legale. Partiamo dall'etica, lo facciamo da un tema molto affascinante, un tema su cui la posta in gioco è molto alta, e che può fornirci degli spunti interessanti, che è quello dei veicoli a guida autonoma. Secondo alcuni esperti, le macchine che si guidano da sole sono la più grande promessa tecnologica del XXI secolo. Molti altri non sono d'accordo, ma ciò che è certo è che si tratta di una tecnologia che può cambiare il mondo, sul serio. Non solo consentendoci di leggere un giornale o di guardare la nostra serie TV preferita mentre siamo alla guida, ammesso che già non lo facciamo e non si fa, ma anche comportando delle vere trasformazioni economiche, culturali, sociali, ambientali. Tutti i soggetti che ora non possono guidare da soli, come i non vedenti, soggetti con delle disabilità motorie, potrebbero avere più autonomia nel movimento. Sarebbero creati e distrutti interi settori dell'economia, cambiando il mondo del lavoro, cambiando gli spostamenti in città, il modo stesso in cui le città sono progettate e collegate tra di loro. Si ridurrebbe il numero di veicoli, quindi il traffico e anche le emissioni nocive, ma soprattutto, e qui arriva l'etica, le auto a guida autonoma ridurranno il numero di incidenti e le vittime, eliminando l'errore umano dall'equazione della guida. Gli incidenti, certo, continueranno ad avvenire in misura ridotta e in questo caso i loro esiti dovranno essere determinati con anticipo dai programmatori, con anni d'anticipo, cioè il tipo di decisione che milioni di auto a guida autonoma si troveranno a prendere nel prossimo futuro riguardano i soggetti da salvare. I programmatori dovranno istruirle a come reagire in situazioni di pericolo, per esempio banalmente, iperbanalizzando se salvare un passeggero o un pedone. E qui mi serve il vostro aiuto. Ipotizziamo che ad un certo punto, in un futuro non troppo lontano, siate sfrecciando sulla vostra auto a guida autonoma e ad un certo punto vi troviate sul percorso un anziano che sta attraversando. L'auto non può fermarsi in tempo per evitare l'impatto e deve prendere una decisione se proseguire dritto e investire l'anziano o se sterzare sull'altra corsia e investire un bambino che sta attraversando da solo dall'altro lato della strada. E allora è vero che è una decisione difficile, atroce, perché coinvolge due vite umane, ma ci sarà un principio decisionale da dover insufflare nella macchina. Che cosa dovrebbe fare quell'automobile? Lo chiedo a voi. Dovrebbe procedere, proseguire dritto e investire l'anziano o sterzare, cambiare corsia e investire il bambino? In quanti per alzata di mano farebbero sterzare l'auto e investirebbero il bambino? Nessuno mi pare di vedere. Ecco, c'è un dato sconcertante. Alcuni studiosi del MIT, del Massachusetts Institute of Technology, elaborano una ricerca che si intitola The Moral Machine, lanciano un sito web che potete visitare, The Moral Machine, e chiedono a decine di milioni di partecipanti in oltre 230 territori in tutto il mondo di esprimere le proprie preferenze morali. Ci sono due immagini, situazioni di emergenza, come quelle che abbiamo appena considerato, che coinvolgono la vita umana e le auto a guida autonoma. E si deve esprimere la propria decisione, la propria preferenza morale. Ecco, raccolgono decine di milioni di decisioni morali e che cosa scoprono? Che in Europa, o più in generale in Occidente, rispondiamo così. Salviamo quasi sempre il bambino. Ma in certi paesi dell'Asia, in certe zone dell'Asia, a questa stessa domanda, bambino o anziano, è molto, molto probabile che ci avrebbero risposto investiamo il bambino e salviamo l'anziano. Perché sembrano avere un rispetto maggiore nei confronti dell'anziano, o quantomeno una cultura dell'anziano differente dalla nostra. Cioè, secondo questo studio, in certe zone dell'Asia, in un cluster geografico, uno dei tre che questi studiosi hanno individuato, che è locato in Asia, non si ha una preferenza così forte nel salvare le vite dei bambini, dei giovani. E questo è un dato intrigante. È un dato intrigante perché ci racconta come la nostra cultura e il nostro background influenzano le decisioni che prendiamo. Ed è un dato intrigante se pensiamo a quello che si chiama problema di allineamento. Cioè, alla necessità di allineare l'intelligenza artificiale ai nostri valori, di trasmettere il valore umano all'intelligenza artificiale. Questo è possibile solo se comprendiamo queste differenze. perché altrimenti il rischio è di valicare il confine, di arrivare in Giappone e di sedere in un'auto a guida autonoma che, scoprendo troppo tardi, ha un'impostazione etica differente dalla nostra. E allora questa è la sfida etica. Qualcuno parla di algo etica, etica dell'algoritmo. Assicurarsi che l'intelligenza artificiale comprenda queste diversità culturali e individuali nelle preferenze morali. Ma, ma, la sfida è anche un'altra. Assicurarsi che l'intelligenza artificiale ci riconosca per quello che siamo, per quelli che siamo, come individui, senza tutti quei pregiudizi e quei preconcetti che abbiamo accumulato come esseri umani. E attenzione, questo è particolarmente importante in quelle intelligenze artificiali che sono coinvolte in decisioni esistenziali. La possibilità di ottenere un mutuo, di ricevere un trattamento sanitario adeguato, di accedere all'università. ecco, se l'intelligenza artificiale è addestrata su dati che contengono bias o che riflettono dei pregiudizi umani, allora può perpetuare quei pregiudizi, risultando in delle decisioni ingiuste o discriminatorie. Due esempi veloci, pregiudizi di genere. Una ricerca condotta negli Stati Uniti ha dimostrato che un algoritmo utilizzato da una grande casa tecnologica per la selezione del personale, preferiva in maniera sistematica i candidati uomini rispetto alle candidate donne. Pregiudizi etnici o razziali, c'era un altro algoritmo utilizzato in ambito giudiziario stavolta, per calcolare il rischio di recidiva dei detenuti, cioè la possibilità che i detenuti ricommettessero certi reati, e che era più incline, si è dimostrato, a categorizzare come ad alto rischio di recidiva i detenuti neri rispetto ai detenuti bianchi. E questo problema si evince tantissimo anche nelle IA generative, text to video, text to image, come quella che ha prodotto la foto del Papa, cioè quei siti in cui inserisci un testo e ti viene restituita una foto. Ecco, molti, molti studi hanno dimostrato questo. In base, sulla base di come sono addestrate queste intelligenze artificiali, dei database su cui vengono allenate, è molto probabile che restituiscano delle immagini fortemente stereotipate, razziste, misogene, omofobe. Per esempio, se chiedete a questi sistemi di generare l'immagine di un soggetto in una posizione di autorità, di un manager, è molto probabile che sia un uomo e un uomo bianco. Se chiedete la foto di un lavoratore domestico, sarà quasi sempre una donna. E come vedete, non una donna bianca. E se chiedete l'immagine di un operaio, o anche di un detenuto, sarà quasi sempre un uomo, e un uomo, come vedete, dalla carnagione scura. Ecco, questi sono i bias che trasferiamo alle macchine. E se davvero questi sistemi ci aiuteranno, come fanno a decidere, a riflettere, anche a generare opere, a creare opere d'arte, a chissà cos'altro in futuro, beh, allora è necessario che siano imparziali. Perché l'intelligenza artificiale può finire non solo per riflettere queste disparità che già esistono, ma addirittura può finire per ampliarle. E queste sono forme di discriminazione, di discriminazione sono potenziali violazioni di diritti umani. E poi c'è l'altro aspetto, bellissimo, quello legale, collezione dei dati, privacy, assicurazione, copyright, diritto d'autore, responsabilità legale. Pensiamo all'anziano che potremmo aver investito sfortunatamente con la nostra autoguida autonoma. chi è responsabile di quella morte? E attenzione, non è una questione filosofica o immaginaria oggigiorno. È il 18 marzo 2018, intorno alle 22 siamo in Arizona, dove durante un test di prova un'auto a guida autonoma investe ed uccide una donna di nome Elena Erzbeck. È la prima fatalità di un pedone che coinvolge un veicolo a guida autonoma. Il veicolo, l'auto, non riesce a riconoscere il soggetto come un pedone, non riesce a prevedere dove stia andando e c'è un guidatore, c'è una persona che funge da backup di sicurezza ma dal video si vince e in quel momento sta guardando un talent show canoro e allora chi è il responsabile di quella morte? È il guidatore di sicurezza? È il pedone? È la macchina stessa? È il produttore della macchina? È lo sviluppatore del software? O è lo Stato? Lo Stato che magari non ha legiferato per tempo? Sono tutte questioni legali e giuridiche ancora aperte, tutte domande che richiedono risposta e le risposte le possiamo e le dobbiamo trovare e lo possiamo fare se investiamo in governance. Che cosa vuol dire? Vuol dire implementare lo sviluppo dell'intelligenza artificiale dandoci delle regole, creando dei meccanismi di diritto. Finora le aziende si sono impegnate a seguire degli standard volontari senza grandi obblighi. Ecco, ora è il momento di darsi delle regole. non per fermare il progresso. Qualcuno dice regolamentare, dare delle regole impedisce alle aziende di fare innovazione. Non è necessariamente così. Regolamentare serve a limitare, a mitigare i rischi. Pensiamo all'intelligenza artificiale come una macchina ancora una volta. Una macchina che corre velocemente, non dobbiamo porre dei blocchi stradali e impedirle di correre, quanto piuttosto dei guardrail ai lati della strada perché possa avanzare anche tanto, tanto velocemente ma entro certi limiti. E' quello che ha fatto di recente l'Unione Europea con l'Artificial Intelligence Act. La prima legge al mondo sull'intelligenza artificiale. Un regolamento molto all'avanguardia che distingue vari livelli di rischio. Il rischio inaccettabile definisce alcuni sistemi che sono completamente vietati. Sistemi di categorizzazione biometrica, di polizia predittiva, di riconoscimento delle emozioni e anche altri e pone degli obblighi di trasparenza in capo ai produttori. Ma quello che ci serve è una collaborazione di tutti gli stati, una governance internazionale perché questa è una sfida che non ha confini geografici e soprattutto è una sfida su un tema su cui c'è una forte, c'è una feroce rivalità tra gli stati. Il 21 marzo 2024 l'Assemblea Generale delle Nazioni Unite ha adottato all'unanimità la prima risoluzione globale sull'intelligenza artificiale. È un primo passo ma la strada è ancora lunga. E allora dicevamo all'inizio è il tempo il tema. Perché? Perché i legislatori hanno ragionevolmente bisogno di tempo per comprendere l'impatto di queste nuove tecnologie prima di poter dare delle regole. E qui sorge un pararosso che nel 1980 David Collingridge aveva teorizzato. Consiste in questo. Lui dice non ha senso dare delle regole immediatamente quando una tecnologia è nuova. Perché? Perché per saggiare il reale impatto di una nuova tecnologia di un'innovazione occorre attendere che si sviluppi che si diffonda. Le norme se io intervengo immediatamente non sono in grado di regolare i risvolti futuri di governare ciò che verrà dopo. E qui sorge il paradosso se invece io aspetto che una tecnologia si faccia radicata sia matura rischio di perdere il potere la capacità di governarla di dare delle regole. Chiudo con un messaggio un messaggio importante secondo me in questi tempi in cui siamo bombardati da notizie negative sul tema da titoli speculativi ad effetto dichiarazioni di politici sul fatto che l'intelligenza artificiale distruggerà l'esistenza umana ecco è vero che mai nella storia ci siamo trovati di fronte a una tecnologia così potente ma è una tecnologia che non ha uno scopo in sé e ha possibile duplice uso benefico o distruttivo è uno strumento neutro qualcuno dice come un coltello da cucina ci puoi tagliare l'arrosto o ci puoi ferire qualcuno è tutto nelle nostre mani allora il tema non è l'intelligenza artificiale ma quella umana l'umanità se l'intelligenza artificiale ci ha portati ad un punto di svolta così epocale allora la responsabilità di realizzarne gli incubi o le promesse aspetta a noi noi che abbiamo creato anche le armi nucleari poi è stata la politica dopo il 55 dopo la guerra a dare vita all'agenzia internazionale per l'energia atomica per garantirne un uso pacifico e sicuro da lì poi sono nate le condizioni per il trattato di non proliferazione nucleare allora siamo noi a porre il nostro futuro nelle mani dei sistemi che creiamo dunque viviamo come se il futuro dipendesse da noi e non dalle macchine come se fosse l'intelligenza umana e non quella artificiale l'unica artefice del nostro destino perché almeno per oggi e ancora per oggi è esattamente così che stanno le cose grazie grazie grazie grazie