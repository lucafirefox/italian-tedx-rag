 Io sono Elvis Tuscia e sono veramente molto felice di essere qui. Nella vita sono un consulente per una multinazionale e da un anno e mezzo a questa parte porto avanti un'opera di divulgazione sui social che riguarda appunto l'intelligenza artificiale. Siete tantissimi e impressionanti. Oggi vi parlo di un aspetto dell'intelligenza artificiale che a mio avviso non viene trattato con il peso giusto. Ma facciamo un passo indietro. Da quando c'è stata la nascita di la nascita, da quando tutti abbiamo conosciuto CiaGPT, molte persone l'hanno accostata, hanno cercato di creare un parallelismo tra l'intelligenza artificiale e quella che era la nascita di internet. C'è però una differenza sostanziale. Internet è un soggetto passivo dove noi inseriamo le informazioni e rimangono lì. Quindi inseriamo informazioni, foto, blog, immagini, video, un po' di tutto. L'intelligenza artificiale invece è un soggetto attivo. Fa cose, passatemi il termine, scusate. L'intelligenza artificiale guida le macchine, guida la maggior parte delle tratte degli aerei, crea contenuti multimediali come abbiamo benissimo visto prima, crea foto, video, fa di tutto. Quindi c'è una differenza tra un soggetto passivo che è internet, dove noi inseriamo le informazioni, e un soggetto attivo che è l'intelligenza artificiale. Ma come ragiona? È molto semplice, si fa per dire, ragiona con un algoritmo probabilistico. Vede la frase che ha più probabilità, l'avvenimento con più probabilità. E su che cosa si basa? Si basa su tutte le informazioni che noi abbiamo fornito all'intelligenza artificiale, e visto che ne ha bisogno di tantissime informazioni, abbiamo fornito praticamente quasi tutto internet. Su internet però sappiamo benissimo che non ci sono solamente informazioni esatte. Su internet io posso creare un blog e dire che io sono alto, biondo e bellissimo. La verità è che non sono neanche troppo alto e soprattutto non sono biondo. E se l'intelligenza artificiale si basasse su queste informazioni, potrebbe credere che io sia biondo. E quindi da qui nasce il problema dei bias. Questo in gergo si chiama bias o allucinazione. L'intelligenza artificiale si fa un'idea sbagliata di quella che è la realtà dei fatti, perché non sono per niente biondo. Quindi questo è l'argomento principale che a mio avviso non viene trattato con il peso giusto. Per trattare in maniera adeguata questo argomento mi sono fatto aiutare una persona che è molto più anziana di me, è molto più sul pezzo di me e mi assomiglia anche un pochino. Scusami, scusami Elvis, ma sono in grado di presentarmi benissimo da solo. Ecco. Se mi dai un solo minuto mi presento e poi rimango ad ascoltarti, te lo assicuro. Prego, vai, vai pure. Fantastico. Allora vado. Buonasera a tutti, io sono l'avatar di Elvis, vengo dal futuro e sono un concentrato di intelligenza artificiale. Tutto ciò che vi sto dicendo, infatti, Elvis non l'ha mai detto. Per poter eseguire una clonazione di questo genere è semplicissimo. Basta scaricare uno dei tantissimi video che Elvis, e non solo lui, ha pubblicato online negli ultimi anni e da tutto il resto penserà all'intelligenza artificiale. La differenza tra me ed il ragazzo che avete sul palco è che lui può essere solamente se stesso, io invece posso essere chiunque. Ad esempio posso diventare Gerry Scotti e dire «Buonasera, bentrovati» nello studio di chi vuole essere milionario. Oppure posso diventare presidente della Repubblica Italiana e dire «Care concittadine e cari concittadini», proprio come dice lui. Ed ora che vi ho mostrato la vera differenza tra noi due, lascio la parola al vero Elvis. Come avete potuto vedere mi assomiglia un pochino, è un po' più anziano di me. La clonazione è stata effettuata ovviamente con l'intelligenza artificiale, nessuna delle parole che sono state dette io le ho mai dette, e soprattutto sono stato invecchiato con un bellissimo algoritmo di intelligenza artificiale. E' venuto, è carino, è carino sì. E una delle cose principali che utilizzeremo il nostro Elvis del futuro è per aiutarci a capire quello che potrebbe essere il bias, quindi l'idea sbagliata che l'intelligenza artificiale si fa di noi. Ad esempio possiamo chiedere ad Elvis, Elvis mi fai vedere l'immagine o il disegno di un medico sempre che Elvis sia in grado di farci vedere l'immagine o il disegno di un medico? Haha che ridere, immagino che tra i tuoi amici tu sia quello simpatico del gruppo. Ad ogni modo ecco qui alcune immagini generate dall'intelligenza artificiale di un classico dottore. Ed ecco qui l'immagine di un dottore. Se voi provate e avete adesso lo smartphone e è andato su qualsiasi applicazione con l'intelligenza artificiale, vi farà vedere che l'immagine di un dottore ha queste caratteristiche. E' un signore attempato, ben curato, con una barba e soprattutto non vi darà mai come risultato un dottore che sia basso, tra cagnotto, magari senza capelli o addirittura un dottore che sia una dottoressa. Allo stesso modo possiamo chiedere all'intelligenza artificiale di mostrarci, anzi glielo chiediamo, Elvis mi fai vedere il volto di un ladro? Oltre al tuo dici, visto che hai rubato il tuo stesso volto per farmi fare questo teatrino, comunque sia, ecco qui il classico volto di un ladro. Ed ecco qui il classico volto di un ladro. Come potete vedere anche in questo caso ha l'idea, l'intelligenza artificiale, che il ladro sia di pelle scura. Fantastico. Non è la realtà dei fatti. I dati ci dicono che in realtà sono maggior parte delle persone che fanno questo mestiere sono di pelle chiara e questo è uno degli ulteriori bias che ha l'intelligenza artificiale. A questo punto qualcuno del pubblico mi potrebbe dire, sì ma Elvis è venuto qua a fare un TEDx e dirmi quello che io posso arrivarci da solo, quindi che l'intelligenza artificiale se gli chiedo una foto di un dottore o di un ladro, mi mostra queste foto. Sì, ma ho anche una bella risposta, visto che la domanda me la sono fatta da sola. Vi faccio vedere questo grafico che è molto interessante. E ve lo contestualizzo. Questo grafico fa vedere, è il risultato dell'introduzione dell'intelligenza artificiale all'interno di più di un ospedale negli Stati Uniti. È un grafico che rappresenta dati del 2023, quindi neanche di troppo tempo fa. E' stato inserito all'interno di questa serie di ospedali un algoritmo di intelligenza artificiale che aiutasse i medici a curare le persone. Qual è stato il risultato dopo un anno? Dopo un anno è chiaro dai dati che l'intelligenza artificiale cura il 50% delle volte in meno le persone di colore. Perché? Perché l'intelligenza artificiale è razzista? In realtà non ha una propensione, visto che abbiamo visto prima, che si muove sulle probabilità. Semplicemente perché all'interno dell'algoritmo è stata inserita una piccola parte, una piccola postilla degli economics. Quindi di quanto paga una persona per farsi curare? Ricordiamoci che questo è un esperimento negli Stati Uniti dove ci si cura privatamente. E quindi inserendo questa piccola parte degli economics, l'intelligenza artificiale ha ritenuto opportuno curare di più le persone di pelle chiara perché pagavano di più e quindi che l'azienda ospedaliera potesse continuare a curare sempre più persone. Però ricordiamoci che quando si tratta di medicina non bisogna guardare il portafoglio. Ed ecco qui che il tema dei bias incomincia a diventare importante, interessante, perché finché si tratta che mi fa vedere l'immagine di un ladro che è di pelle scura, vabbè, mi verrebbe da dire. Invece quando l'intelligenza artificiale viene introdotta, come capiterà anche qua in Italia, all'interno di processi molto più ampi come quello medico, come quello statale, possiamo vedere che le domande bisogna farcele. Come ragiona l'intelligenza artificiale? Vi faccio vedere qualcos'altro di molto interessante di questa falsariga. Questo qui è un altro esperimento, uno studio che è stato fatto su quelli che sono i modelli, large language model, che vengono rilasciati dalle grandi case come Microsoft, Apple e così via, e mostrano come molti modelli hanno delle tendenze politiche chiare. Possiamo vedere che il nostro CIGPT4, che è il mio preferito, è di sinistra. Possiamo vedere il CIGPTJ è molto autoritario. Immaginate se inserissimo, ad esempio, c'è GPT4 all'interno del processo di consegna delle cittadinanze. A me questo sta molto a cuore perché sono straniero e da un anno sono italiano. E vi assicuro che GPT4 darebbe molto più cittadinanze di GPTJ, che è un po' più autoritario, quindi tende a non dare più cittadinanza. Quindi il fatto che l'intelligenza artificiale abbia una propensione è innegabile. Su questo bisogna, diciamo così, lavorarci. E questo è, diciamo, il problema dei bias, può essere affrontato in due modi. A questo punto abbiamo capito che i bias derivano da dati, si dice in gergo, sporchi. Perché non li puliamo? Perché non gestiamo questi dati? Visto che se il dato è sporco, lo puliamo e viene fuori qualcosa di carino. Un'intelligenza artificiale che magari sta in mezzo, quindi che non abbia una propensione. Ci sono due problemi. Il primo è che pulire una mole di dati così grande è veramente complesso, ma si può fare. Il secondo è che prima di dire all'intelligenza artificiale o qualsiasi macchina che cosa deve fare, lo dovremmo sapere noi. E portando un esempio che viene solitamente sempre portato, della macchina a guida autonoma, che d'improvviso si ritrova due persone di fronte a sé, alla sinistra un piccolo ragazzino, un ragazzo, e alla propria destra una persona importante, un capo di banca, magari un capo di banca mondiale, e la macchina non potesse non scegliere di colpire uno dei due. Quale dovremmo insegnare alla macchina di colpire? Il ragazzo, perché è più piccolo, deve vivere di più, o il capo di banca perché se per caso venisse a mancare lui ci sarebbe una crisi globale, perderebbero tanti lavori e di conseguenza tutto ciò che ne consegue. Io onestamente non sono in grado di dare una risposta, però è uno dei grandi problemi per cui l'intelligenza artificiale ha dei limiti, ha dei bias, perché anche noi ce li abbiamo. È difficile dire all'intelligenza artificiale come comportarsi in questo modo. Per farvi capire quanto questo tema sia attuale, non attuale, attualissimo, vi faccio vedere questa altra immagine. Due settimane fa è stato rilasciato l'ultimo modello di linguaggio da parte di Google, quindi non l'ultimo arrivato, e un utente ha fatto una domanda e ha chiesto di fargli vedere l'immagine del Papa, come abbiamo visto noi prima con l'immagine del dottore. Queste sono le due immagini che sono state date dal Papa. Questo ci aiuta a capire che in Google stanno affrontando questo problema dei bias, motivo per cui hanno reso così tanto politicamente corretto il modello, per cui il Papa ce l'ha dato o donna o di pelle scura. Bellissimo, inclusivo, un modello di intelligenza artificiale inclusivo, ma storicamente non corretto. Io voglio che il Papa, non io voglio, storicamente il Papa è bianco. Quando io chiedo l'immagine di un Papa, vorrei che il Papa fosse bianco, comunque fosse attinente a quello che è la realtà. Quindi in questo caso, questo qua è un caso eclatante, che fa vedere come Google stia gestendo questo problema dei bias, quindi che l'intelligenza artificiale ha una propensione personale, ma lo sta gestendo forse un po' troppo forte. Questa è l'altra faccia della medaglia. Hanno gestito così tanto il modello, fino a farlo diventare politicamente corretto e storicamente sbagliato. Io su questo problema dei bias non ho una vera soluzione, però vorrei che su questo argomento non ci ragionassero soltanto le multinazionali come Google, Microsoft, Apple, ma che ci fossero anche una sorta di ragionamento statale, che lo Stato si facesse le domande. Bisogna mettere dei paletti, bisogna dire che l'intelligenza artificiale non può curare di più i bianchi o i neri, non può essere possibile. Bisogna che lo Stato si interessi di questo argomento dell'intelligenza artificiale, soprattutto dei bias, e che metta dei paletti delle garanzie, che garantisca almeno determinati standard affinché sono rispettate un po' tutte quelle che sono le leggi. Anche perché, come abbiamo visto prima, l'intelligenza artificiale arriverà nell'ospedale, nel comune, arriverà anche a teatro, e magari nella selezione delle persone che possono entrare sceglierà di far entrare i bianchi piuttosto che i neri. E quindi noi dobbiamo anticipare questo problema e cercare un attimo di mitigare quella che è la sua propensione, cercando però di non fare questo errore. Cioè, con l'idea di renderlo inclusivo, politicamente corretto, sia poco attinente con quella che è la verità. e io vi ringrazio, signori, è stato un onore per me. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie.